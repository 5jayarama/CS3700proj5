#!/usr/bin/env python3

import argparse
import socket
import ssl
import re
import sys
from html.parser import HTMLParser
from parser1 import Parser
import random

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.socket = None
        self.csrftoken = ""
        self.session_id = ""
        self.urls_to_visit = set()
        self.visited_urls = set()

    # main run method
    def run(self):
        # Fakebook uses HTTPS, so TLS over TCP
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # added encryption socket connection with TLS and decoding
        self.socket = ssl.create_default_context().wrap_socket(mysocket, server_hostname=self.server)
        self.socket.connect((self.server, self.port))
        # open login page, make get request, and organize it
        request = f'GET /accounts/login/?next=/fakebook/ HTTP/1.1\r\nHost:{self.server}:{self.port}\r\n\r\n'
        get_response = self.send(request)
        get_response_split = re.split("\r\n", get_response)
        # print("Get response: ", get_response_split)

        # locate the csrf token cookie from the response
        login_cookie = ""
        for line in get_response_split:
            if len(line) >= 10 and line[0:10] == "set-cookie":
                # print("Line with csrftoken: ", line)
                words = re.split("\s", line) # split by whitespace
                for word in words:
                    if len(word) >= 9 and word[0:9] == "csrftoken": #look for word "csrftoken=<long number>"
                        login_cookie = word[0:len(word) - 1]
        # print("Login cookie: ", login_cookie)

        # parse the HTML data to extract the CSRF token
        parser = Parser("csrf")
        html = re.split("\r\n\r\n", get_response)[1]
        parser.feed(html)
        # print("parser results: ", parser.results)

        # use parser to retrieve the token
        login_token = parser.get_csrf_token()
        if login_token == "Error":
            print("Error: No login token")
            sys.exit(1)

        # create a POST request
        post_data = f"username={self.username}&password={self.password}&csrfmiddlewaretoken={login_token}&next=/fakebook/"
        post_data_len = len(post_data)
        request = f'''POST /accounts/login/?next=/fakebook/ HTTP/1.1\r\nHost: {self.server}\r\nCookie: {login_cookie}\r\nContent-Length: {post_data_len}\r\nContent-Type: application/x-www-form-urlencoded\r\n\r\n{post_data}\r\n\r\n'''
        # print("Post response Not split: ", self.send(request))
        # split the response and get the csrf token and session id
        post_response_split = re.split("\r\n", self.send(request))
        # print("Post Response: ", post_response_split)
        for line in post_response_split:
            if len(line) >= 10 and line[0:10] == "set-cookie":
                # print("Line with csrftoken: ", line)
                words = re.split("\s", line)
                if line[12:21] == "csrftoken":
                    for word in words:
                        if len(word) >= 9 and word[0:9] == "csrftoken":
                            self.csrftoken = word[0:len(word) - 1]
                if line[12:21] == "sessionid":
                    for word in words:
                        if len(word) >= 9 and word[0:9] == "sessionid":
                            self.session_id = word[0:len(word) - 1]
        # print("Self session id: ", self.session_id)
        # print("Self csrf token: ", self.csrftoken) 
        # redirecting to fakebook page (fixed previous 302 error)
        request = f'GET /fakebook/ HTTP/1.1\r\nHost:{self.server}:{self.port}\r\n' \
                  f'Cookie: {self.csrftoken}; {self.session_id}\r\n\r\n'
        # print("second get request: ", request)
        login_response = re.split("\r\n\r\n", self.send(request))

        # login response of 200 means successfully login
        # print("login response: ", login_response[0])
        p = Parser("url")
        p.feed(login_response[1])
        # print("Parser results: ", p.results)
        self.get_linked_urls(p.results["href"])
        self.crawl()


    # moved the send code from run here
    def send(self, request):
        self.socket.send(request.encode('ascii'))
        data = self.socket.recv(4096).decode('utf-8')
        # code below was added for 1.1 chunked responses: if data is chunked, call dechunk
        if "Transfer-Encoding: chunked" in data:
            data = self.dechunk(data)
        return data
    
    # created this method for reusing sockets
    def send_crawl(self, request):
        self.socket.send(request.encode('ascii'))
        # Initial read to get headers
        headers = ''
        while '\r\n\r\n' not in headers:
            headers += self.socket.recv(1024).decode('utf-8')
        
        # find where the headers end
        header_end_index = headers.find('\r\n\r\n') + 4
        
        # extract Content-Length
        content_length = 0
        for line in headers.split('\r\n'):
            if line.lower().startswith('content-length:'):
                content_length = int(line.split(":")[1].strip())
                break

        # calculate bytes already received
        already_received = len(headers) - header_end_index
        # initialize body
        body = headers[header_end_index:]
        
        # read the rest of the body based on content_length
        while already_received < content_length:
            chunk = self.socket.recv(content_length - already_received)
            body += chunk.decode('utf-8')
            already_received += len(chunk)
        response = headers[:header_end_index] + body
        return response # changed from returning header, body


    # dechunk by removing every other line in the body
    def dechunk(self, data):
        data_list = re.split("\r\n\r\n", data) # split the data into a header and body
        # create a new body where we skip every other line of the old body (include only lines 0, 2, 4, etc.)
        new_body = ""
        i = 0
        for body_line in re.split("\r\n", data_list[1]):
            if i % 2 == 0:
                new_body += body_line
            i += 1
        return data_list[0] + "\r\n\r\n" + new_body
    
    def get_linked_urls(self, urls):
        for url in urls:
            # Ensure the URL meets your criteria before adding
            if url.startswith("/fakebook/"):
                # Add the URL if it's not already visited or in the queue
                if url not in self.visited_urls and url not in self.urls_to_visit:
                    self.urls_to_visit.add(url)

    # after logging in, crawl through fakebook to find the necessary flags
    def crawl(self):
        flags_found = 0
        i = 0
        if not self.urls_to_visit:
            print("ran out of urls")
        while self.urls_to_visit: # while there are still flags to find
            i += 1
            next_url = self.urls_to_visit.pop()
            # print("next url: ", next_url, flush=True)
            # print("flags found: ", flags_found, flush=True)
            if next_url in self.visited_urls:
                continue
            self.visited_urls.add(next_url)
           
            # create a get request and record the response
            request = f"GET {next_url} HTTP/1.1\r\n" + \
                    f"Host: {self.server}\r\n" + \
                    f"Cookie: {self.csrftoken}; {self.session_id}\r\n" + \
                    "Connection: keep-alive\r\n" + \
                    "\r\n"
            # print("crawl request: ", request)
            response = self.send_crawl(request)
            # print("Resp:", response)

            if "302" in response: # 302 FOUND
                # Try the request again using the new URL given by the server in the Location header
                headers = re.split("\r\n", response)
                redirect = ""
                for header in headers:
                    if len(header) >= 8 and header[0:8] == "Location":
                        redirect = header[10:]
                self.get_linked_urls([redirect])
                continue
            
            # if 403 or 404, just abondon url and enter loop again
            elif "403" in response or "404" in response:
                continue
            # program should not get to anything below here
            elif "500" in response or "504" in response:
                continue

            # use parser to find the flag
            flag_parser = Parser("flag")
            flag_parser.feed(response)
            # if the flag is found using parser, then print it
            if "flag" in flag_parser.results:
                print(flag_parser.results["flag"][6:])
                flags_found += 1
                if flags_found == 5:
                    return
            # create a url parser to find more urls
            url_parser = Parser("url")
            url_parser.feed(response)
            # print("finding new urls")
            if "href" in url_parser.results:
                self.get_linked_urls(url_parser.results["href"])
            else:
                continue
            
            if "Connection: close" in response:
                mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                # added encryption socket connection with TLS and decoding
                self.socket = ssl.create_default_context().wrap_socket(mysocket, server_hostname=self.server)
                self.socket.connect((self.server, self.port))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
