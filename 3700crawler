#!/usr/bin/env python3

import argparse
import socket
import ssl
import re
import sys
from html.parser import HTMLParser
from parser1 import Parser

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.socket = None
        self.csrftoken = ""
        self.session_id = ""
        self.urls_to_visit = []
        self.visited_urls = []

    # main run method
    def run(self):

        # open login page, make get request, and organize it
        request = f'GET /accounts/login/?next=/fakebook/ HTTP/1.1\r\nHost:{self.server}:{self.port}\r\n\r\n'
        get_response = self.send(request)
        get_response_split = re.split("\r\n", get_response)
        # print("Get response: ", get_response_split)

        # locate the csrf token cookie from the response
        login_cookie = ""
        for line in get_response_split:
            if len(line) >= 10 and line[0:10] == "set-cookie":
                # print("Line with csrftoken: ", line)
                words = re.split("\s", line) # split by whitespace
                for word in words:
                    if len(word) >= 9 and word[0:9] == "csrftoken": #look for word "csrftoken=<long number>"
                        login_cookie = word[0:len(word) - 1]
        # print("Login cookie: ", login_cookie)

        # parse the HTML data to extract the CSRF token
        parser = Parser("csrf")
        html = re.split("\r\n\r\n", get_response)[1]
        parser.feed(html)
        # print("parser results: ", parser.results)

        # use parser to retrieve the token
        login_token = parser.get_csrf_token()
        if login_token == "Error":
            print("Error: No login token")
            sys.exit(1)

        # create a POST request
        post_data = f"username={self.username}&password={self.password}&csrfmiddlewaretoken={login_token}&next=/fakebook/"
        post_data_len = len(post_data)
        request = f'''POST /accounts/login/?next=/fakebook/ HTTP/1.1\r\nHost: {self.server}\r\nCookie: {login_cookie}\r\nContent-Length: {post_data_len}\r\nContent-Type: application/x-www-form-urlencoded\r\n\r\n{post_data}\r\n\r\n'''
        # print("Post response Not split: ", self.send(request))
        # split the response and get the csrf token and session id
        post_response_split = re.split("\r\n", self.send(request))
        # print("Post Response: ", post_response_split)
        for line in post_response_split:
            if len(line) >= 10 and line[0:10] == "set-cookie":
                # print("Line with csrftoken: ", line)
                words = re.split("\s", line)
                if line[12:21] == "csrftoken":
                    for word in words:
                        if len(word) >= 9 and word[0:9] == "csrftoken":
                            self.csrftoken = word[0:len(word) - 1]
                if line[12:21] == "sessionid":
                    for word in words:
                        if len(word) >= 9 and word[0:9] == "sessionid":
                            self.session_id = word[0:len(word) - 1]
        # print("Self session id: ", self.session_id)
        # print("Self csrf token: ", self.csrftoken) 
        # redirecting to fakebook page (fixed previous 302 error)
        request = f'GET /fakebook/ HTTP/1.1\r\nHost:{self.server}:{self.port}\r\n' \
                  f'Cookie: {self.csrftoken}; {self.session_id}\r\n\r\n'
        # print("second get request: ", request)
        login_response = re.split("\r\n\r\n", self.send(request))

        # login response of 200 means successfully login
        # print("login response: ", login_response[0])
        p = Parser("url")
        p.feed(login_response[1])
        # print("Parser results: ", p.results)
        self.get_linked_urls(p.results["href"])
        self.crawl()


    # moved the send code from run here
    def send(self, request):
        # Fakebook uses HTTPS, so TLS over TCP
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # added encryption socket connection with TLS and decoding
        self.socket = ssl.create_default_context().wrap_socket(mysocket, server_hostname=self.server)
        self.socket.connect((self.server, self.port))
        self.socket.send(request.encode('ascii'))
        data = self.socket.recv(50000).decode('utf-8')
        # code below was added for 1.1 chunked responses: if data is chunked, call dechunk
        data_split = re.split("\r\n", data)
        # print("Data_split: ", data_split)
        for data_line in data_split:
            if len(data_line) >= 18 and data_line[0:17] == "Transfer-Encoding":
                if "chunked" in re.split("\s", data_line):
                    return self.dechunk(data)
        return data
    
    # dechunk by removing every other line in the body
    def dechunk(self, data):
        data_list = re.split("\r\n\r\n", data) # split the data into a header and body
        # create a new body where we skip every other line of the old body (include only lines 0, 2, 4, etc.)
        new_body = ""
        i = 0
        for body_line in re.split("\r\n", data_list[1]):
            if i % 2 == 0:
                new_body += body_line
            i += 1
        return data_list[0] + "\r\n\r\n" + new_body
    
    # get all the urls that start with /fakebook/
    def get_linked_urls(self, urls):
        filtered_urls = []
        for url in urls:
            # print("New URL: ", url)
            if len(url) >= 10 and url[:10] == "/fakebook/":
                filtered_urls.append(url)
        self.urls_to_visit.extend(filtered_urls)

    # after logging in, crawl through fakebook to find the necessary flags
    def crawl(self):
        flags_found = 0
        while self.urls_to_visit: # while there are still flags to find
            # look for the next url to crawl through which has not been visited 
            for url in self.urls_to_visit:
                if url not in self.visited_urls:
                    next_url = url
                    self.urls_to_visit.remove(next_url)
                    break
            if next_url is not None:
                self.visited_urls.append(next_url)
           
            # create a get request and record the response
            request = f"GET {next_url} HTTP/1.1\r\n"
            headers = f"Host: {self.server}:{self.port}\r\nCookie: {self.csrftoken}; {self.session_id}\r\nConnection: " \
                      f"keep-alive\r\n\r\n "
            request += headers
            # print("crawl request: ", request)
            response = self.send(request)
            # print("New page response: ", response)

            # split response into body and header.
            header, body = response.split("\r\n\r\n")
            # print("crawling header ", header)
            # print("crawling body", body)

            # determine next steps based on header
            if "200" in header: # 200 OK
                # use parser to find the flag
                flag_parser = Parser("flag")
                flag_parser.feed(body)
                # if the flag is found using parser, then print it
                if "flag" in flag_parser.results:
                    print(flag_parser.results["flag"][6:])
                    flags_found += 1
                    if flags_found == 5:
                        return
                # create a url parser to find more urls
                url_parser = Parser("url")
                url_parser.feed(body)
                # print("finding new urls")
                if (body):
                    self.get_linked_urls(url_parser.results["href"])
    
            elif "302" in header: # 302 FOUND
                # Try the request again using the new URL given by the server in the Location header
                headers = re.split("\r\n", header)
                redirect = ""
                for header in headers:
                    if len(header) >= 8 and header[0:8] == "Location":
                        redirect = header[10:]
                self.get_linked_urls([redirect])
                continue
            
            # if 403 or 404, just abondon url and enter loop again
            elif "403" in header or "404" in header:
                continue
            # program should not get to anything below here
            elif "500" in header or "504" in header:
                continue
            else:
                continue

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
